{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uGENyIKC-de",
        "outputId": "2f57015d-0792-4771-927e-b7311a3a312a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy3 in /usr/local/lib/python3.11/dist-packages (2.0.3)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (2.4.417150.4580142)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Догружаем то, чего нет в Коллабе"
      ],
      "metadata": {
        "id": "QUQhWNgQDBht"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qDPAfoCx9GkU"
      },
      "outputs": [],
      "source": [
        "\n",
        "import nltk\n",
        "import pymorphy3\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from pymorphy3 import MorphAnalyzer\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `import nltk` – Подключает библиотеку Natural Language Toolkit (NLTK) для обработки естественного языка.\n",
        "\n",
        "2. `from nltk.stem import SnowballStemmer` – Импортирует алгоритм стемминга Snowball, который приводит слова к их основам.\n",
        "\n",
        "3. `from nltk.tokenize import word_tokenize` – Импортирует функцию для разбиения текста на отдельные слова (токенизация).\n",
        "\n",
        "4. `from pymorphy3 import MorphAnalyzer` – Импортирует мощный морфологический анализатор для работы с русским языком.\n",
        "\n",
        "5. `import string` – Подключает модуль для работы со строками и знаками пунктуации."
      ],
      "metadata": {
        "id": "Fy98v3XJ9Xvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNh_a1Ye9qHd",
        "outputId": "94775e08-7b73-4272-e8cf-ee5e2465569c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Первое Загружает модуль токенизации `punkt_tab`, который заменил `punkt` в последних версиях NLTK.\n",
        "\n",
        "Второе . `nltk.download('wordnet')` – Загружает базу WordNet, используемую для работы с синонимами и лемматизацией английских слов."
      ],
      "metadata": {
        "id": "di97ytyI9s85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Надо мною тишина, небо полное дождя, Дождь проходит сквозь меня, но боли больше нет, под холодный шепот звезд, мы сожгли последний мост, и все в бездну сорвалось, свободным стану я , от зла и от добра, моя душа была на лезвии ножа.'\n",
        "english_text = 'Somebody once told me the world is gonna roll me, i aint the sharpest tool in the shack, she was looking kind of dumb with a finger and a thumb in a shape of an L on her forehead.'"
      ],
      "metadata": {
        "id": "E6jnwOXC9sO-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выше указан текст, который будет подвергнут дальнейшей работе"
      ],
      "metadata": {
        "id": "3df614Wy-L7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lematize(text: str) -> list[str]:\n",
        "    morph3 = MorphAnalyzer()\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized_words = [morph3.parse(word)[0].normal_form for word in tokens]\n",
        "    return lemmatized_words\n"
      ],
      "metadata": {
        "id": "6rBaE4Af-ORN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `def lematize(text: str) -> list[str]:` – Определяет функцию `lematize`, которая принимает строку `text` и возвращает список лемматизированных слов.\n",
        "\n",
        "2. `morph3 = MorphAnalyzer()` – Создаёт экземпляр морфологического анализатора `pymorphy3`, который будет использоваться для приведения слов к их начальной форме.\n",
        "\n",
        "3. `tokens = word_tokenize(text)` – Разбивает входной текст на отдельные слова (токены) с помощью `word_tokenize` из NLTK.\n",
        "\n",
        "4. `lemmatized_words = [morph3.parse(word)[0].normal_form for word in tokens]` – Проходит по каждому токену, анализирует его с `MorphAnalyzer` и извлекает его начальную форму (`normal_form`).\n",
        "\n",
        "5. `return lemmatized_words` – Возвращает список лемматизированных слов."
      ],
      "metadata": {
        "id": "FfI2TnoM-PZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def stemming(text: str) -> list[str]:\n",
        "    stemmer = SnowballStemmer(\"russian\")\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized_words = [stemmer.stem(word) for word in tokens]\n",
        "    return lemmatized_words"
      ],
      "metadata": {
        "id": "95jN4odW-P1V"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `def stemming(text: str) -> list[str]:` – Определяет функцию `stemming`, которая принимает строку `text` и возвращает список слов после стемминга.\n",
        "\n",
        "2. `stemmer = SnowballStemmer(\"russian\")` – Создаёт экземпляр стеммера Snowball для русского языка, который будет использоваться для приведения слов к их основе.\n",
        "\n",
        "3. `tokens = word_tokenize(text)` – Разбивает входной текст на отдельные слова (токены) с помощью `word_tokenize` из NLTK.\n",
        "\n",
        "4. `lemmatized_words = [stemmer.stem(word) for word in tokens]` – Проходит по каждому токену и применяет к нему стемминг (`stem()`), который отбрасывает окончания и приводит слово к его основе.\n",
        "\n",
        "5. `return lemmatized_words` – Возвращает список стеммированных слов."
      ],
      "metadata": {
        "id": "WuvP0SUS-SwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ascii_tokenizer(text: str) -> list[str]:\n",
        "    return [char for char in text if char in string.printable]"
      ],
      "metadata": {
        "id": "W2JdAUDA-RWO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `def ascii_tokenizer(text: str) -> list[str]:` – Определяет функцию `ascii_tokenizer`, которая принимает строку `text` и возвращает список символов, содержащихся в строке и являющихся печатными.\n",
        "\n",
        "2. `return [char for char in text if char in string.printable]` – Генерирует список символов, которые присутствуют в строке `text` и являются печатными символами (определяется через `string.printable`).\n",
        "\n",
        "3. `string.printable` включает все символы, которые могут быть напечатаны, включая буквы, цифры, знаки препинания и пробел."
      ],
      "metadata": {
        "id": "1O8rYJWM-V-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ascii_vectorizer(text: str) -> list[str]:\n",
        "    return [ord(char) for char in text if char in string.printable]\n"
      ],
      "metadata": {
        "id": "X-YU8A01-VVC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `def ascii_vectorizer(text: str) -> list[str]:` – Определяет функцию `ascii_vectorizer`, которая принимает строку `text` и возвращает список числовых значений (кодировок) символов, являющихся печатными.\n",
        "\n",
        "2. `return [ord(char) for char in text if char in string.printable]` – Генерирует список числовых значений (кодов ASCII) для каждого символа в строке `text`, если этот символ является печатным (определяется через `string.printable`).\n",
        "\n",
        "3. Функция `ord(char)` преобразует символ в его числовой код (например, 'A' → 65).\n",
        "\n",
        "4. Результат — это список числовых представлений печатных символов из строки `text`."
      ],
      "metadata": {
        "id": "8kePWD89-aOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tokenize(text: str) -> list[str]:\n",
        "    return word_tokenize(text)"
      ],
      "metadata": {
        "id": "bwE_9y35-Z3s"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `def tokenize(text: str) -> list[str]:` – Определяет функцию `tokenize`, которая принимает строку `text` и возвращает список токенов (слов).\n",
        "\n",
        "2. `return word_tokenize(text)` – Использует функцию `word_tokenize` из NLTK для разбиения текста на отдельные слова (токены).\n",
        "\n",
        "3. Результат — это список слов, полученных из строки `text` с помощью токенизации."
      ],
      "metadata": {
        "id": "q1Xid9am-c4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(tokens: list[str]) -> list[int]:\n",
        "    dict_vectors = {}\n",
        "    result = []\n",
        "    for word in tokens:\n",
        "        if word in dict_vectors.keys():\n",
        "            result.append(dict_vectors[word])\n",
        "        else:\n",
        "            dict_vectors[word] = len(dict_vectors)\n",
        "            result.append(dict_vectors[word])\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "KRdidTJO-chw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `def vectorize(tokens: list[str]) -> list[int]:` – Определяет функцию `vectorize`, которая принимает список токенов `tokens` и возвращает список числовых векторов (индексов) для каждого токена.\n",
        "\n",
        "2. `dict_vectors = {}` – Создаёт пустой словарь `dict_vectors`, который будет хранить уникальные слова и их индексы.\n",
        "\n",
        "3. `result = []` – Инициализирует пустой список `result`, в который будут добавляться числовые представления (индексы) слов.\n",
        "\n",
        "4. `for word in tokens:` – Проходит по каждому слову в списке токенов.\n",
        "\n",
        "5. `if word in dict_vectors.keys():` – Проверяет, существует ли уже это слово в словаре `dict_vectors`.\n",
        "\n",
        "6. `result.append(dict_vectors[word])` – Если слово уже есть в словаре, добавляет его индекс в список `result`.\n",
        "\n",
        "7. `else:` – Если слово не найдено в словаре, добавляет его в словарь и присваивает ему новый индекс, равный текущей длине словаря.\n",
        "\n",
        "8. `dict_vectors[word] = len(dict_vectors)` – Присваивает уникальный индекс слову, добавляя его в словарь.\n",
        "\n",
        "9. `result.append(dict_vectors[word])` – Добавляет индекс нового слова в список `result`.\n",
        "\n",
        "10. `return result` – Возвращает список индексов для каждого слова из входного списка токенов.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jdOJKrvh-eTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Лемматизация:')\n",
        "print(lematize(text))\n",
        "\n",
        "print('Стемминг:')\n",
        "print(stemming(text))\n",
        "\n",
        "print('Токенизация всех символов из ASCII:')\n",
        "print(ascii_tokenizer(english_text))\n",
        "\n",
        "print('Векторизация всех символов из ASCII:')\n",
        "print(ascii_vectorizer(english_text))\n",
        "\n",
        "print('Векторизация текста после лемматизации:')\n",
        "print(vectorize(lematize(text)))\n",
        "\n",
        "print('Векторизация текста после стемминга:')\n",
        "print(vectorize(stemming(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWxrBNdl-enQ",
        "outputId": "2efb373d-5105-48cb-aa9f-e0003e271aa6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лемматизация:\n",
            "['надо', 'я', 'тишина', ',', 'небо', 'полный', 'дождь', ',', 'дождь', 'проходить', 'сквозь', 'я', ',', 'но', 'боль', 'большой', 'нет', ',', 'под', 'холодный', 'шёпот', 'звезда', ',', 'мы', 'сжечь', 'последний', 'мост', ',', 'и', 'всё', 'в', 'бездна', 'сорваться', ',', 'свободный', 'стать', 'я', ',', 'от', 'зло', 'и', 'от', 'добро', ',', 'мой', 'душа', 'быть', 'на', 'лезвие', 'нож', '.']\n",
            "Стемминг:\n",
            "['над', 'мно', 'тишин', ',', 'неб', 'полн', 'дожд', ',', 'дожд', 'проход', 'сквоз', 'мен', ',', 'но', 'бол', 'больш', 'нет', ',', 'под', 'холодн', 'шепот', 'звезд', ',', 'мы', 'сожгл', 'последн', 'мост', ',', 'и', 'все', 'в', 'бездн', 'сорва', ',', 'свободн', 'стан', 'я', ',', 'от', 'зла', 'и', 'от', 'добр', ',', 'мо', 'душ', 'был', 'на', 'лезв', 'нож', '.']\n",
            "Токенизация всех символов из ASCII:\n",
            "['S', 'o', 'm', 'e', 'b', 'o', 'd', 'y', ' ', 'o', 'n', 'c', 'e', ' ', 't', 'o', 'l', 'd', ' ', 'm', 'e', ' ', 't', 'h', 'e', ' ', 'w', 'o', 'r', 'l', 'd', ' ', 'i', 's', ' ', 'g', 'o', 'n', 'n', 'a', ' ', 'r', 'o', 'l', 'l', ' ', 'm', 'e', ',', ' ', 'i', ' ', 'a', 'i', 'n', 't', ' ', 't', 'h', 'e', ' ', 's', 'h', 'a', 'r', 'p', 'e', 's', 't', ' ', 't', 'o', 'o', 'l', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 's', 'h', 'a', 'c', 'k', ',', ' ', 's', 'h', 'e', ' ', 'w', 'a', 's', ' ', 'l', 'o', 'o', 'k', 'i', 'n', 'g', ' ', 'k', 'i', 'n', 'd', ' ', 'o', 'f', ' ', 'd', 'u', 'm', 'b', ' ', 'w', 'i', 't', 'h', ' ', 'a', ' ', 'f', 'i', 'n', 'g', 'e', 'r', ' ', 'a', 'n', 'd', ' ', 'a', ' ', 't', 'h', 'u', 'm', 'b', ' ', 'i', 'n', ' ', 'a', ' ', 's', 'h', 'a', 'p', 'e', ' ', 'o', 'f', ' ', 'a', 'n', ' ', 'L', ' ', 'o', 'n', ' ', 'h', 'e', 'r', ' ', 'f', 'o', 'r', 'e', 'h', 'e', 'a', 'd', '.']\n",
            "Векторизация всех символов из ASCII:\n",
            "[83, 111, 109, 101, 98, 111, 100, 121, 32, 111, 110, 99, 101, 32, 116, 111, 108, 100, 32, 109, 101, 32, 116, 104, 101, 32, 119, 111, 114, 108, 100, 32, 105, 115, 32, 103, 111, 110, 110, 97, 32, 114, 111, 108, 108, 32, 109, 101, 44, 32, 105, 32, 97, 105, 110, 116, 32, 116, 104, 101, 32, 115, 104, 97, 114, 112, 101, 115, 116, 32, 116, 111, 111, 108, 32, 105, 110, 32, 116, 104, 101, 32, 115, 104, 97, 99, 107, 44, 32, 115, 104, 101, 32, 119, 97, 115, 32, 108, 111, 111, 107, 105, 110, 103, 32, 107, 105, 110, 100, 32, 111, 102, 32, 100, 117, 109, 98, 32, 119, 105, 116, 104, 32, 97, 32, 102, 105, 110, 103, 101, 114, 32, 97, 110, 100, 32, 97, 32, 116, 104, 117, 109, 98, 32, 105, 110, 32, 97, 32, 115, 104, 97, 112, 101, 32, 111, 102, 32, 97, 110, 32, 76, 32, 111, 110, 32, 104, 101, 114, 32, 102, 111, 114, 101, 104, 101, 97, 100, 46]\n",
            "Векторизация текста после лемматизации:\n",
            "[0, 1, 2, 3, 4, 5, 6, 3, 6, 7, 8, 1, 3, 9, 10, 11, 12, 3, 13, 14, 15, 16, 3, 17, 18, 19, 20, 3, 21, 22, 23, 24, 25, 3, 26, 27, 1, 3, 28, 29, 21, 28, 30, 3, 31, 32, 33, 34, 35, 36, 37]\n",
            "Векторизация текста после стемминга:\n",
            "[0, 1, 2, 3, 4, 5, 6, 3, 6, 7, 8, 9, 3, 10, 11, 12, 13, 3, 14, 15, 16, 17, 3, 18, 19, 20, 21, 3, 22, 23, 24, 25, 26, 3, 27, 28, 29, 3, 30, 31, 22, 30, 32, 3, 33, 34, 35, 36, 37, 38, 39]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результат работы"
      ],
      "metadata": {
        "id": "GUksqZMn-hDA"
      }
    }
  ]
}