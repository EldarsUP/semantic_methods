{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPPyz2PskuF1riwK95cjUR7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EldarsUP/semantic_methods/blob/main/homework3%7C3final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "1Ay7KOGkj3wq",
        "outputId": "4cdda009-0030-4873-9596-ff6e17fc8c8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 2.6455\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-f52cfc14d0a8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "# 1. Загрузка текста\n",
        "with open(\"/content/gpt_data (1).txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Создание уникальных символов + специальные токены\n",
        "chars = ['<BOS>', '<EOS>'] + sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Маппинг символов в индексы и наоборот\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "\n",
        "# Функции кодирования и декодирования\n",
        "encode = lambda s: [stoi.get(c, stoi['<EOS>']) for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Кодируем текст (с добавлением специальных токенов)\n",
        "encoded_data = encode(\"<BOS> \" + text + \" <EOS>\")\n",
        "data = torch.tensor(encoded_data, dtype=torch.long)\n",
        "\n",
        "# 3. Параметры\n",
        "block_size = 128\n",
        "batch_size = 32\n",
        "embedding_dim = 128\n",
        "n_heads = 4\n",
        "n_layers = 2\n",
        "ff_hidden = 256\n",
        "n_epochs = 80\n",
        "dropout_rate=0.1\n",
        "lr = 3e-4\n",
        "\n",
        "# 4. Dataset\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx:idx + self.block_size]\n",
        "        y = self.data[idx + 1:idx + self.block_size + 1]\n",
        "        return x, y\n",
        "\n",
        "dataloader = DataLoader(CharDataset(data, block_size), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 5. Модель\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.q = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        q = self.q(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.k(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.v(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        att = att.masked_fill(torch.tril(torch.ones(T, T, device=att.device)) == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.dropout(att)\n",
        "        out = (att @ v).transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.out(out)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_hidden):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_hidden, embed_dim),\n",
        "        )\n",
        "        self.ln2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class MiniGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, block_size, n_heads, n_layers, ff_hidden):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.position_embedding = nn.Embedding(block_size, embed_dim)\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            TransformerBlock(embed_dim, n_heads, ff_hidden)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.shape\n",
        "        tok_emb = self.token_embedding(x)\n",
        "        pos_emb = self.position_embedding(torch.arange(T, device=x.device))[None, :, :]\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# 6. Обучение\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MiniGPT(vocab_size, embedding_dim, block_size, n_heads, n_layers, ff_hidden).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for x, y in dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        B, T, C = logits.shape\n",
        "        loss = F.cross_entropy(logits.view(B * T, C), y.view(B * T))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == n_epochs - 1:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "def generate(model, start_text=\"Привет\", max_new_tokens=100, temperature=1.0, top_k=None):\n",
        "    model.eval()\n",
        "    context = torch.tensor(encode(start_text), dtype=torch.long)[None, :].to(device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        context_condensed = context[:, -block_size:]\n",
        "        logits = model(context_condensed)\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        if top_k is not None:\n",
        "            values, indices = torch.topk(probs, top_k)\n",
        "            probs = torch.zeros_like(probs).scatter_(1, indices, values)\n",
        "            probs /= probs.sum(dim=-1, keepdim=True)\n",
        "\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "        context = torch.cat((context, next_token), dim=1)\n",
        "\n",
        "    return decode(context[0].tolist())\n",
        "\n",
        "print(\"\\n=== Сгенерированный текст ===\")\n",
        "print(generate(model, start_text=\"А вы ноктюрн сыграть смогли бы \", max_new_tokens=200, temperature=0.7, top_k=10))\n"
      ]
    }
  ]
}