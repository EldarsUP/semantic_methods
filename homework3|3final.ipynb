{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOXcyS8BTf2c8nBdmTBGlMA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EldarsUP/semantic_methods/blob/main/homework3%7C3final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ay7KOGkj3wq",
        "outputId": "82a44904-4025-4483-d4d0-8b5efe53a84b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 2.5954\n",
            "Epoch 10, Loss: 1.5939\n",
            "Epoch 20, Loss: 1.1618\n",
            "Epoch 30, Loss: 1.1152\n",
            "Epoch 40, Loss: 1.0406\n",
            "Epoch 50, Loss: 0.9410\n",
            "Epoch 60, Loss: 0.9439\n",
            "Epoch 70, Loss: 0.8533\n",
            "Epoch 79, Loss: 0.9491\n",
            "\n",
            "=== Сгенерированный текст ===\n",
            "А вы ноктюрн сыграть смогли бы отными краными размывами, и скраными рамочными цилинниками; горловины на шарнирах были сложены пополам, как сетую двагентом в бортироваться. Вешь обочине.\n",
            "\n",
            "– Всё простор день из гредумал. Всё, бы орто\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "# 1. Загрузка текста\n",
        "with open(\"/gpt_data (1).txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Создание уникальных символов + специальные токены\n",
        "chars = ['<BOS>', '<EOS>'] + sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Маппинг символов в индексы и наоборот\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "\n",
        "# Функции кодирования и декодирования\n",
        "encode = lambda s: [stoi.get(c, stoi['<EOS>']) for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Кодируем текст (с добавлением специальных токенов)\n",
        "encoded_data = encode(\"<BOS> \" + text + \" <EOS>\")\n",
        "data = torch.tensor(encoded_data, dtype=torch.long)\n",
        "\n",
        "# 3. Параметры\n",
        "block_size = 128\n",
        "batch_size = 32\n",
        "embedding_dim = 128\n",
        "n_heads = 4\n",
        "n_layers = 2\n",
        "ff_hidden = 256\n",
        "n_epochs = 80\n",
        "dropout_rate=0.1\n",
        "lr = 3e-4\n",
        "\n",
        "# 4. Dataset\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx:idx + self.block_size]\n",
        "        y = self.data[idx + 1:idx + self.block_size + 1]\n",
        "        return x, y\n",
        "\n",
        "dataloader = DataLoader(CharDataset(data, block_size), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 5. Модель\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.q = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        q = self.q(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.k(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.v(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        att = att.masked_fill(torch.tril(torch.ones(T, T, device=att.device)) == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.dropout(att)\n",
        "        out = (att @ v).transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.out(out)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_hidden):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_hidden, embed_dim),\n",
        "        )\n",
        "        self.ln2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class MiniGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, block_size, n_heads, n_layers, ff_hidden):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.position_embedding = nn.Embedding(block_size, embed_dim)\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            TransformerBlock(embed_dim, n_heads, ff_hidden)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.shape\n",
        "        tok_emb = self.token_embedding(x)\n",
        "        pos_emb = self.position_embedding(torch.arange(T, device=x.device))[None, :, :]\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# 6. Обучение\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MiniGPT(vocab_size, embedding_dim, block_size, n_heads, n_layers, ff_hidden).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for x, y in dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        B, T, C = logits.shape\n",
        "        loss = F.cross_entropy(logits.view(B * T, C), y.view(B * T))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == n_epochs - 1:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "def generate(model, start_text=\"Привет\", max_new_tokens=100, temperature=1.0, top_k=None):\n",
        "    model.eval()\n",
        "    context = torch.tensor(encode(start_text), dtype=torch.long)[None, :].to(device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        context_condensed = context[:, -block_size:]\n",
        "        logits = model(context_condensed)\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        if top_k is not None:\n",
        "            values, indices = torch.topk(probs, top_k)\n",
        "            probs = torch.zeros_like(probs).scatter_(1, indices, values)\n",
        "            probs /= probs.sum(dim=-1, keepdim=True)\n",
        "\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "        context = torch.cat((context, next_token), dim=1)\n",
        "\n",
        "    return decode(context[0].tolist())\n",
        "\n",
        "print(\"\\n=== Сгенерированный текст ===\")\n",
        "print(generate(model, start_text=\"А вы ноктюрн сыграть смогли бы \", max_new_tokens=200, temperature=0.7, top_k=10))\n"
      ]
    }
  ]
}