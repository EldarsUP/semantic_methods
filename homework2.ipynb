{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOK+JHeNxLWD5sQuLLdaguR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EldarsUP/semantic_methods/blob/main/homework2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Догружаем нужные библиотеки"
      ],
      "metadata": {
        "id": "dhwsxOO1_m21"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJienj8c_XMU",
        "outputId": "2ba81659-9b4c-4ad0-a0bd-0cb295f00756"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy3\n",
            "  Downloading pymorphy3-2.0.3-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Downloading pymorphy3-2.0.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.3 pymorphy3-dicts-ru-2.4.417150.4580142\n"
          ]
        }
      ],
      "source": [
        "pip install pymorphy3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импортируем нужные для работы библиотеки"
      ],
      "metadata": {
        "id": "qHP4sWWq_siM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymorphy3\n",
        "import math\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5iuD_rJ_9V7",
        "outputId": "1ed0b13f-1864-464b-f842-a4ef6e5ba506"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция Лемматизации"
      ],
      "metadata": {
        "id": "kWp5W0SlA3Cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(text):\n",
        "  morph = pymorphy3.MorphAnalyzer()\n",
        "  tokens = word_tokenize(text)\n",
        "  return [morph.parse(word)[0].normal_form for word in tokens if not word in set(stopwords.words(\"russian\"))]\n",
        "\n",
        "\n",
        "text = \"Шла саша по шоссе и сосала сушку\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(re.sub(\"[^\\w\\s]\", \"\",\" \".join(lemmatize(text))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi69h472A8s5",
        "outputId": "ce728398-015b-4f6f-a310-4641dd8732b0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "идти саша шоссе сосать сушка\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенизация + Лемматизация+ удаление стоп слов и пунктуации"
      ],
      "metadata": {
        "id": "HGmCn3SbBWfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  tokens = word_tokenize(text)\n",
        "  return tokens\n",
        "\n",
        "text = [\"Не говорите мне 'прощай'. Я помню как мы повстречались.Я целый день провел в печали. И вечер предвещал тоску.\",\n",
        "\t\"Не говорите мне 'Прощай'.Я помню как нашел случайно, Глаза которые печально Искали лето на снегу\",\n",
        "\t\"Не говорите мне 'прощай', Я не хочу расстаться с вами, Я не хочу играть словами,И вы поверьте мне сейчас. не говорите мне 'прощай', Скажите только что согласны, Сказать одно лишь слово 'Здравствуй'-Ведь это мой последний шанс.\",\n",
        "\t\"Не говорите мне 'прощай'- Я это слово ненавижу, Я вас нисколько не обижу, Руки коснувшись невзначай. Не говорите мне 'прощай',не верю я что вы бездушны,Мне ничего от вас не нужно -Не говорите мне 'прощай'.\",\n",
        "\t\"Не говорите мне 'прощай', не говорите, В глаза еще раз откровенно посмотрите, Согрейте сердце мне, прошу вас, в трудный час, Не представляю, как я буду жить без вас. Моя судьба сейчас на волоске: Я замок свой построил на песке.\"]\n",
        "\n",
        "lemm_text = [tokenize(re.sub(\"[^\\w\\s]\", \"\",\" \".join(lemmatize(t)))) for t in text]\n",
        "print(lemm_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6ftU6wTBYHS",
        "outputId": "a0ee809b-bd51-469b-ca3e-f1f9a56ee060"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['не', 'говорить', 'прощать', 'я', 'помнить', 'повстречалисьть', 'целый', 'день', 'провести', 'печаль', 'и', 'вечер', 'предвещать', 'тоска'], ['не', 'говорить', 'прощайть', 'помнить', 'найти', 'случайно', 'глаз', 'который', 'печально', 'искать', 'лето', 'снег'], ['не', 'говорить', 'прощать', 'я', 'хотеть', 'расстаться', 'вы', 'я', 'хотеть', 'играть', 'слово', 'и', 'поверьте', 'говорить', 'прощать', 'сказать', 'согласный', 'сказать', 'один', 'лишь', 'слово', 'здравствуйведь', 'это', 'последний', 'шанс'], ['не', 'говорить', 'прощай', 'я', 'это', 'слово', 'ненавидеть', 'я', 'нисколько', 'обидеть', 'рука', 'коснуться', 'невзначай', 'не', 'говорить', 'прощать', 'верить', 'бездушный', 'я', 'нужно', 'не', 'говорить', 'прощать'], ['не', 'говорить', 'прощать', 'говорить', 'в', 'глаз', 'откровенно', 'посмотреть', 'согреть', 'сердце', 'просить', 'трудный', 'час', 'не', 'представлять', 'быть', 'жить', 'мой', 'судьба', 'волосок', 'я', 'замок', 'свой', 'построить', 'песок']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Строим матрицу частотности слов(для начала формируем множество уникальных слов)"
      ],
      "metadata": {
        "id": "4S4ngYsXFAWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "res = [item for sublist in lemm_text for item in sublist]\n",
        "print(set(res))\n",
        "\n",
        "def word_frequency_matrix(texts, unique_words):\n",
        "    word_index = {word: i for i, word in enumerate(unique_words)}\n",
        "\n",
        "    matrix = []\n",
        "    for text in texts:\n",
        "      row = [0] * len(unique_words)\n",
        "      for word in text:\n",
        "          row[word_index[word]] += 1\n",
        "      matrix.append(row)\n",
        "\n",
        "    return matrix\n",
        "\n",
        "matrix = word_frequency_matrix(lemm_text, set(res))\n",
        "for row in matrix:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yv6w8NgYEePO",
        "outputId": "6ee00f14-29c8-4f06-9718-e5ab249e786d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'глаз', 'вы', 'обидеть', 'расстаться', 'лето', 'нисколько', 'повстречалисьть', 'согласный', 'трудный', 'час', 'искать', 'поверьте', 'мой', 'случайно', 'нужно', 'просить', 'прощать', 'верить', 'хотеть', 'лишь', 'и', 'здравствуйведь', 'невзначай', 'откровенно', 'волосок', 'посмотреть', 'не', 'день', 'рука', 'свой', 'сердце', 'ненавидеть', 'вечер', 'найти', 'шанс', 'слово', 'снег', 'представлять', 'прощай', 'который', 'предвещать', 'один', 'последний', 'я', 'это', 'целый', 'коснуться', 'судьба', 'помнить', 'говорить', 'бездушный', 'жить', 'быть', 'построить', 'замок', 'песок', 'провести', 'печаль', 'в', 'печально', 'сказать', 'играть', 'тоска', 'прощайть', 'согреть'}\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0]\n",
            "[1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0]\n",
            "[0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 2, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 1, 2, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0]\n",
            "[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 3, 1, 0, 1, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 2, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Рассчет TF-IDF с использованием логарифмов\n",
        "IDF = LOG(N/DF+1), n- кол-во текстов, DF кол-во текстов со словом"
      ],
      "metadata": {
        "id": "nZl24Op9Fm4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем словарь для подсчета, сколько текстов содержит каждое уникальное слово\n",
        "word_index = {word: i for i, word in enumerate(set(res))}\n",
        "non_zero_counts = [0] * len(word_index)  # Инициализируем список нулями\n",
        "\n",
        "# Обход всех текстов\n",
        "for text in lemm_text:\n",
        "    unique_in_text = set(text)  # Берем только уникальные слова из текста\n",
        "    for word in unique_in_text:\n",
        "        non_zero_counts[word_index[word]] += 1  # Увеличиваем счетчик\n",
        "# Исправленный расчет IDF\n",
        "idf_values = [math.log(len(lemm_text) / (df + 1)) for df in non_zero_counts]  # Добавляем +1 для стабильности\n",
        "\n",
        "# Исправленная функция TF-IDF\n",
        "def tf_idf(texts, matrix, idf_values, unique_words):\n",
        "    word_index = {word: i for i, word in enumerate(unique_words)}\n",
        "    tf_matrix = []\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "        len_text = len(text) if len(text) > 0 else 1  # Длина текста для нормализации TF\n",
        "        row = [0] * len(unique_words)\n",
        "        for word in text:\n",
        "            tf = matrix[i][word_index[word]] / len_text  # Правильный TF\n",
        "            idf = idf_values[word_index[word]]  # Правильный IDF\n",
        "            row[word_index[word]] = tf * idf  # TF-IDF\n",
        "        tf_matrix.append(row)\n",
        "\n",
        "    return tf_matrix\n",
        "\n",
        "# Применяем исправленную функцию\n",
        "matrix1 = tf_idf(lemm_text, matrix, idf_values, set(res))\n",
        "\n",
        "# Вывод результата\n",
        "for row in matrix1:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NY0mzgjrGDWy",
        "outputId": "817d10bf-d7bb-4696-9af3-dbb3e3a1bc07"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0.06544933799101108, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0, 0, 0, 0.03648754455471362, 0, 0, 0, 0, 0, -0.013022968342425327, 0.06544933799101108, 0, 0, 0, 0, 0.06544933799101108, 0, 0, 0, 0, 0, 0, 0, 0.06544933799101108, 0, 0, 0.0, 0, 0.06544933799101108, 0, 0, 0.03648754455471362, -0.013022968342425327, 0, 0, 0, 0, 0, 0, 0.06544933799101108, 0.06544933799101108, 0, 0, 0, 0, 0.06544933799101108, 0, 0]\n",
            "[0.04256880198049923, 0, 0, 0, 0.07635756098951292, 0, 0, 0, 0, 0, 0.07635756098951292, 0, 0, 0.07635756098951292, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.015193463066162882, 0, 0, 0, 0, 0, 0, 0.07635756098951292, 0, 0, 0.07635756098951292, 0, 0, 0.07635756098951292, 0, 0, 0, 0, 0, 0, 0, 0, 0.04256880198049923, -0.015193463066162882, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.07635756098951292, 0, 0, 0, 0.07635756098951292, 0]\n",
            "[0, 0.03665162927496621, 0, 0.03665162927496621, 0, 0, 0, 0.03665162927496621, 0, 0, 0, 0.03665162927496621, 0, 0, 0, 0, 0.0, 0, 0.07330325854993242, 0.03665162927496621, 0.02043302495063963, 0.03665162927496621, 0, 0, 0, 0, -0.0072928622717581834, 0, 0, 0, 0, 0, 0, 0, 0.03665162927496621, 0.04086604990127926, 0, 0, 0, 0, 0, 0.03665162927496621, 0.03665162927496621, 0.0, 0.02043302495063963, 0, 0, 0, 0, -0.014585724543516367, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.07330325854993242, 0.03665162927496621, 0, 0, 0]\n",
            "[0, 0, 0.039838727472789354, 0, 0, 0.039838727472789354, 0, 0, 0, 0, 0, 0, 0, 0, 0.039838727472789354, 0, 0.0, 0.039838727472789354, 0, 0, 0, 0, 0.039838727472789354, 0, 0, 0, -0.023781072625298424, 0, 0.039838727472789354, 0, 0, 0.039838727472789354, 0, 0, 0, 0.022209809728956118, 0, 0, 0.039838727472789354, 0, 0, 0, 0, 0.0, 0.022209809728956118, 0, 0.039838727472789354, 0, 0, -0.023781072625298424, 0.039838727472789354, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0.02043302495063963, 0, 0, 0, 0, 0, 0, 0, 0.03665162927496621, 0.03665162927496621, 0, 0, 0.03665162927496621, 0, 0, 0.03665162927496621, 0.0, 0, 0, 0, 0, 0, 0, 0.03665162927496621, 0.03665162927496621, 0.03665162927496621, -0.014585724543516367, 0, 0, 0.03665162927496621, 0.03665162927496621, 0, 0, 0, 0, 0, 0, 0.03665162927496621, 0, 0, 0, 0, 0, 0.0, 0, 0, 0, 0.03665162927496621, 0, -0.014585724543516367, 0, 0.03665162927496621, 0.03665162927496621, 0.03665162927496621, 0.03665162927496621, 0.03665162927496621, 0, 0, 0.03665162927496621, 0, 0, 0, 0, 0, 0.03665162927496621]\n"
          ]
        }
      ]
    }
  ]
}